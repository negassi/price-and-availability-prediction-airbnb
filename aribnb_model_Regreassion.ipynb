{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         Negassi Tesfay\n",
    "##                                                 Airbnb Data Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#!pip install textblob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airbnb_data_wrangling.html\n",
      "airbnb_data_wrangling.ipynb\n",
      "airbnb_eda.html\n",
      "airbnb_eda.ipynb\n",
      "aribnb_model_Regreassion.ipynb\n",
      "aribnb_model_classification.html\n",
      "aribnb_model_classification.ipynb\n",
      "clean_listings.csv\n",
      "listings_ab.csv\n",
      "requirements.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data <br>\n",
    "\n",
    "- Cleaned data from data wrangling will be loaded <br>\n",
    "- Raw data will be loaded to bring the text/comment column we removed. This will be used for NLP processing/sentimental analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('clean_listings.csv',low_memory=False)\n",
    "# only description column for sentimental analysis\n",
    "df_senti=pd.read_csv('listings_ab.csv',low_memory=False)[['id','description']] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two data sets with  Left join (choose rows only with the cleaned data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12679, 47)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all=pd.merge(left=df_clean, right=df_senti, how='left', left_on='id', right_on='id')\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the comment column into sentimental value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    try:\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return score['compound']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "sentiment= df_all.description.apply(lambda x:sentiment_analyzer_scores(x))\n",
    "\n",
    "df_all['sentiment']=sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Columns <br>\n",
    "\n",
    "Columns were splitted into:\n",
    "\n",
    "- Categorical and numeric for imputation purpose(if there are any) and be used as feature variables for modeling.\n",
    "- Id column\n",
    "- description (to be removed)\n",
    "- response varables (change the variable response to price or availabilty to predict respectively)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'price'\n",
    "id_var = 'id'\n",
    "sent_var='description'\n",
    "categoricalVariables = []\n",
    "numericVariables=[]\n",
    "\n",
    "for col in df_all.columns:\n",
    "    if (col not in [id_var, sent_var,response]):\n",
    "        if df_all[col].dtypes == 'object':\n",
    "            categoricalVariables.append(col)\n",
    "        else:\n",
    "            numericVariables.append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to impute missing values, and encode categrorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),  \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([ \n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            (\"ohe\", OneHotEncoder(sparse=False))\n",
    "                        ])\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numericVariables),\n",
    "        (\"cat\", cat_pipeline, categoricalVariables),\n",
    "        \n",
    "    ])\n",
    "\n",
    "df_model = full_pipeline.fit_transform(df_all[numericVariables + categoricalVariables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12679, 48)\n",
      "(12679, 156)\n"
     ]
    }
   ],
   "source": [
    "print(df_all.shape)\n",
    "print(df_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_model, \n",
    "                                                    df_all[response], test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10143, 156)\n",
      "(10143,)\n",
      "(2536, 156)\n",
      "(2536,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance Metrics <br>\n",
    "Model performance will be evaluated with Root Mean Squared Error (RMSE) and the following function calculates RMSE and will set it as choice of metrics. Less RMSE will be desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rmse(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))\n",
    "\n",
    "my_scorer = make_scorer(my_rmse, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate models <br>\n",
    "\n",
    "The following models will be tried and for prediction.<br>\n",
    "- Linear Regression\n",
    "- Decision Tree Regression\n",
    "- Random Forest\n",
    "- Gradiant Boost Regressro\n",
    "- Adaboost Regressor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "forest_reg = RandomForestRegressor( random_state=42)\n",
    "gbm = GradientBoostingRegressor()\n",
    "adaBoost = AdaBoostRegressor(random_state=42, n_estimators=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_reg = LinearRegression()\n",
    "test_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([154.25153562, 220.90231534,  35.75098103, 176.7032797 ,\n",
       "       114.80182859, 186.83261319, 146.6718237 , 180.40326139,\n",
       "       148.02728482, 166.06463071])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=test_reg.predict(X_test)\n",
    "pred[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch /Hyper parametrization<br>\n",
    "To fine tune our model, we will iterate through differtnt values of the parameters of their respective models. The following is the parameter grid of the differtn models ind dictionary data structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lin_reg' :{\n",
    "        'normalize' : [True,False ]\n",
    "    },\n",
    "    \n",
    "    'tree_reg' :{\n",
    "        'criterion': ['mse',\"friedman_mse\"],\n",
    "        'max_features': [\"auto\", \"sqrt\", \"log2\"], \n",
    "    },\n",
    "    \n",
    "    'forest_reg' :{\n",
    "        'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8],\n",
    "        'bootstrap': [False],  \n",
    "    },\n",
    "    'gbm' :{\n",
    "        'n_estimators': [3, 10, 30], \n",
    "      'max_features': [2, 3, 4]\n",
    "    },\n",
    "    \n",
    "    'ada' :{\n",
    "        'n_estimators': [3, 10, 30], \n",
    "      \n",
    "    }\n",
    "    \n",
    "    \n",
    "   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm loading ...\n",
      "forest_reg loading ...\n",
      "tree_reg loading ...\n",
      "lin_reg loading ...\n",
      "ada loading ...\n"
     ]
    }
   ],
   "source": [
    "models = {\"gbm\": gbm, \n",
    "          \"forest_reg\": forest_reg ,\n",
    "          \"tree_reg\":tree_reg,\n",
    "           \"lin_reg\":lin_reg,\n",
    "         \"ada\" : adaBoost\n",
    "         }\n",
    "best_models = {}\n",
    "for m in models.keys():\n",
    "    print('%s loading ...' % m)\n",
    "    grid = GridSearchCV(models[m], param_grid[m], cv=10, scoring=my_scorer)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_models[m] = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above procedure tunes the hyper parameters of each learners and put the resulting models in a dictionary. What follows is comparison of different learnes to choose a champion model. For this purpose, I will use RMSE as a metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model                Train                 Test\n",
      "                 gbm    75.15987512697652    72.44260554609002\n",
      "          forest_reg    64.40527507562089    61.85268193956737\n",
      "            tree_reg    88.99704449728222    87.57088780170353\n",
      "             lin_reg   225440.26058569085     92864.2477784006\n",
      "                 ada    75.13746385587531    71.92250680116622\n"
     ]
    }
   ],
   "source": [
    "print ('%20s %20s %20s' % ('Model','Train','Test'))\n",
    "for m in best_models.keys():\n",
    "    m_ = best_models[m]\n",
    "    rmse_train = np.abs(m_.best_score_)\n",
    "    test_pred = m_.predict(X_test)\n",
    "    rmse_test = my_rmse(test_pred, y_test)\n",
    "    #print(m,rmse_train, rmse_test)\n",
    "    print ('%20s %20s %20s' % (m, rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest model performed better than the other models as it predicted the test data with a the least root mean squared error. Hence I would choose this model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
