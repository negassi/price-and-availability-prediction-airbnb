{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                         Negassi Tesfay\n",
    "##                                                 Airbnb Data Challenge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install vaderSentiment\n",
    "#!pip install textblob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from math import sqrt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airbnb_data_wrangling.html\n",
      "airbnb_data_wrangling.ipynb\n",
      "airbnb_eda.html\n",
      "airbnb_eda.ipynb\n",
      "aribnb_model_Regreassion.ipynb\n",
      "aribnb_model_classification.ipynb\n",
      "clean_listings.csv\n",
      "listings_ab.csv\n",
      "requirements.yaml\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data <br>\n",
    "\n",
    "- Cleaned data from data wrangling will be loaded <br>\n",
    "- Raw data will be loaded to bring the text/comment column we removed. This will be used for NLP processing/sentimental analysis.\n",
    "- Convert availability to binary(available=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('clean_listings.csv',low_memory=False)\n",
    "# only description column for sentimental analysis\n",
    "df_senti=pd.read_csv('listings_ab.csv',low_memory=False)[['id','description']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['availability_30']=df_clean['availability_30'].apply(lambda x: 0 if x==0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the two data sets with  Left join (choose rows only with the cleaned data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12679, 47)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all=pd.merge(left=df_clean, right=df_senti, how='left', left_on='id', right_on='id')\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the comment column into sentimental value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment_analyzer_scores(sentence):\n",
    "    try:\n",
    "        score = analyser.polarity_scores(sentence)\n",
    "        return score['compound']\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "\n",
    "sentiment= df_all.description.apply(lambda x:sentiment_analyzer_scores(x))\n",
    "\n",
    "df_all['sentiment']=sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Columns <br>\n",
    "\n",
    "Columns were splitted into:\n",
    "\n",
    "- Categorical and numeric for imputation purpose(if there are any) and be used as feature variables for modeling.\n",
    "- Id column\n",
    "- description (to be removed)\n",
    "- response varables ('availability_30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'availability_30'\n",
    "id_var = 'id'\n",
    "sent_var='description'\n",
    "categoricalVariables = []\n",
    "numericVariables=[]\n",
    "\n",
    "for col in df_all.columns:\n",
    "    if (col not in [id_var, sent_var,response]):\n",
    "        if df_all[col].dtypes == 'object':\n",
    "            categoricalVariables.append(col)\n",
    "        else:\n",
    "            numericVariables.append(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to impute missing values, and encode categrorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),  \n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "cat_pipeline = Pipeline([ \n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            (\"ohe\", OneHotEncoder(sparse=False))\n",
    "                        ])\n",
    "\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, numericVariables),\n",
    "        (\"cat\", cat_pipeline, categoricalVariables),\n",
    "        \n",
    "    ])\n",
    "\n",
    "df_model = full_pipeline.fit_transform(df_all[numericVariables + categoricalVariables])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12679, 48)\n",
      "(12679, 156)\n"
     ]
    }
   ],
   "source": [
    "print(df_all.shape)\n",
    "print(df_model.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Train and Test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_model, \n",
    "                                                    df_all[response], test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10143, 156)\n",
      "(10143,)\n",
      "(2536, 156)\n",
      "(2536,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidate models <br>\n",
    "\n",
    "The following models will be tried and for prediction.<br>\n",
    "\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Gradient Boost Classifier\n",
    "- Adaboost Classifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch /Hyper parametrization tunning<br>\n",
    "To fine tune our model, we will iterate through differtnt values of the parameters of their respective models. The following is the parameter grid of the differtn models ind dictionary data structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {   \n",
    "    'tree_clf' :{\n",
    "        'criterion': [\"gini\",\"entropy\"],\n",
    "        'max_features': [\"auto\", \"sqrt\", \"log2\"], \n",
    "    },\n",
    "    \n",
    "    'forest_clf' :{\n",
    "        'n_estimators': [3, 10, 30], 'max_features': [\"auto\", \"sqrt\", \"log2\"],\n",
    "        'bootstrap': [False,True],  \n",
    "    },\n",
    "    'gbm' :{\n",
    "        'n_estimators': [3, 10, 30], \n",
    "      'max_features': [2,3,4]\n",
    "    },\n",
    "    \n",
    "    'ada' :{\n",
    "        'n_estimators': [10,50,100], \n",
    "      \n",
    "    }\n",
    "    \n",
    "    \n",
    "   }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier()\n",
    "forest_clf = RandomForestClassifier()\n",
    "gbm = GradientBoostingClassifier()\n",
    "adaBoost = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbm loading ...\n",
      "forest_clf loading ...\n",
      "tree_clf loading ...\n",
      "ada loading ...\n"
     ]
    }
   ],
   "source": [
    "models = {\"gbm\": gbm, \n",
    "          \"forest_clf\": forest_clf ,\n",
    "          \"tree_clf\":tree_clf,\n",
    "           \"ada\" : adaBoost\n",
    "         }\n",
    "best_models = {}\n",
    "for m in models.keys():\n",
    "    print('%s loading ...' % m)\n",
    "    grid = GridSearchCV(models[m], param_grid[m], cv=5)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_models[m] = grid\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above procedure tunes the hyper parameters of each learners and put the resulting models in a dictionary. What follows is comparison of different learnes to choose a champion model. For this purpose, I will use accuracy as a metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Model                Train                 Test \n",
      "                 gbm                0.711                0.705\n",
      "          forest_clf                  1.0                0.772\n",
      "            tree_clf                  1.0                0.689\n",
      "                 ada                0.779                0.777\n"
     ]
    }
   ],
   "source": [
    "print ('%20s %20s %20s ' % ('Model','Train','Test'))\n",
    "for m in best_models.keys():\n",
    "    m_ = best_models[m]\n",
    "    m_.fit(X_train,y_train)\n",
    "    score_test = m_.score(X_test,y_test)\n",
    "    score_train = m_.score(X_train,y_train)\n",
    "    #print(m,rmse_train, rmse_test)\n",
    "    print ('%20s %20s %20s' % (m, round(score_train,3),round(score_test,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though forest_clf looks to have one of the highest accuracy, it overfits and as a result doesn't generalise well. Consequently, I will choose ada boost as the champion model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PathRiseEnv (Python 3.6)",
   "language": "python",
   "name": "pathriseenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
